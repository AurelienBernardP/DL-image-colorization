Loss network cat

1)

Loss 10 epochs 
batch size = 4
initial_lr = 3e-5
weight_decay=1e-3

[8747.10343153 7746.477937   7718.74370108 7692.08886235 7681.17076184
 7669.92944858 7643.75135628 7644.28186405 7635.46926071 7626.9339383]
 
 --> lr = 3e-05
 
 
 -------------------------------------------------------------------------
 
 2)
 
 108167.15341797  64568.20807292  62108.45804036  61464.31679687
  61033.94342448  60864.89908854  60552.13710937  60328.39270833
  60192.15566406  60201.45712891  60056.10854492  59959.44420573
  59824.57330729  59692.45865885  59700.20524089  59489.76165365
  59455.16103516  59299.09078776  59225.29658203  59075.08925781
  58878.12226563
  
 3)
 
 [105516.95224609  63938.8804362   61836.50498047  61029.84586589
  60920.3890625   60745.07740885  60663.47089844  60666.0843099
  60673.85361328  60674.36074219  60633.48561198  60624.16936849
  60605.7851237   60645.23369141  60648.64003906      0.
      0.              0.              0.              0.        ]
      
  4) 
  
  cat dataset
  
 Loss 15 epochs 
 batch size = 32
 initial_lr = 0.0001
 weight_decay= 0
  
 [81303.28811849 61758.39309896 60933.99667969 60846.84921875
 60542.11933594 60462.38883464 60337.95807292 60110.13154297
 60108.94763997 59988.05817057 59838.87047526 59823.27936198
 59862.82783203 59862.36497396 59503.20800781]
 
 Loss 30 epochs 
 batch size = 32
 initial_lr = 0.0001
 weight_decay= 0
 
 [60385.47617188 59762.79378255 59433.95097656 59476.44599609
 59232.75934245 59243.96279297 59289.17327474 59171.0921224
 59111.21907552 58940.96315104 58793.42688802 58777.8258138
 58615.96347656 58376.17075195 58321.16580404 58087.07001953
 57749.02513021 57416.25751953 57088.15449219 56712.53020833
 56157.13948568 55397.20680339 54623.96822917 53934.46074219
 52274.30084635 51122.04365234 49193.14459635 47385.42890625
 45765.42220052 43749.64643555]
 
 Loss 100 epochs 
 batch size = 32
 initial_lr = 0.0001
 weight_decay= 0
 
 [41399.53181966 39328.01105143 37910.80113932 36703.84072266
 35591.55758464 34562.16321615 33845.62635091 32878.93821615
 31838.24560547 30979.11360677 30132.19482422 29355.42911784
 28645.84597982 28013.73204753 27258.69205729 26525.13854167
 26080.20646159 25697.09316406 24990.82750651 24478.30818685
 23913.17114258 23590.73916016 22796.37157389 22394.54726562
 22202.75438639 21732.13532715 21263.52696126 20803.80828451
 20537.92703451 20218.08628743 19662.33267415 19308.37285156
 19077.41974284 18815.43061523 18515.57627767 18321.58497721
 18255.83783366 18051.08092448 17616.18653971 17315.96066081
 17042.43725586 16933.93890788 16612.2267334  16231.99694824
 15920.14764811 15764.53597819 15789.13213704 15678.92716471
 15457.65901693 15176.34157715 15186.51732585 15100.40015462
 14798.8755778  14589.73163249 14438.51653239]
 
 Loss 130 epochs 
 batch size = 32
 initial_lr = 0.0001
 weight_decay= 0
 
 14595.17989095 13581.99746094 13875.56790365 14093.27469076
 14142.8011556  13910.21253255 13567.58085124 13326.37622477
 13202.69063314 13150.1250651  12940.26027832 12669.48104655
 12581.39984538 12591.53572184 12676.23735352 12542.54065755
 12228.427889   12027.60822754 11967.03070068 11768.7973348
 11741.22125651 11685.82579753 11644.20712077 11495.3114502
 11197.87001953 11004.38503011 10918.35761719 11003.60126546
 11006.58417562 10899.27492269
 
 

 
 
 
 
 

