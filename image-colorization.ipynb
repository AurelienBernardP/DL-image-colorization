{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image colorization project :\n",
    "We train a CNN to take in greyscale images of ... and output their colorful and plausible colorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports required :\n",
    "\n",
    "torch\n",
    "\n",
    "skimage ? only used for rgb - lab parsing. Maybe we can do that ourselves to make some differentiation with the original project?\n",
    "\n",
    "numpy\n",
    "\n",
    "matplotlib\n",
    "\n",
    "PIL ? only used to open images to rgb. We can use another library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "#from skimage import color\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data, augment it, transform it to LAB compute stats on colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LAB transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Color stats (auxiliary functions)\n",
    "\n",
    "def getDiscretisedColor(a,b,gridSize):\n",
    "    a = np.round(a/gridSize) * gridSize\n",
    "    b = np.round(b/gridSize) * gridSize\n",
    "    return (a,b)\n",
    "\n",
    "def getMatrixIndex(a,b,gridSize):\n",
    "    i = (a + 500) / gridSize\n",
    "    j = (b + 200) / gridSize\n",
    "    return (int(i),int(j))\n",
    "\n",
    "def getColorValue(i,j,gridSize):\n",
    "    a = i * gridSize - 500\n",
    "    b = j * gridSize - 200\n",
    "    return (a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[79.2654367 , 22.30273916],\n",
      "        [37.570884  ,  5.43822558],\n",
      "        [93.59548743, 63.10022606]],\n",
      "\n",
      "       [[70.31910002, 98.67446835],\n",
      "        [ 4.6902829 , 67.73261924],\n",
      "        [33.3431722 ,  5.55458338]],\n",
      "\n",
      "       [[77.97145479, 19.7122801 ],\n",
      "        [59.35973549, 16.13164548],\n",
      "        [73.73307333, 52.35854979]]])]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333\n",
      "[(-40, 20), (-40, 30), (-40, 40), (-40, 50), (-40, 60), (-30, -10), (-30, 0), (-30, 10), (-30, 20), (-30, 30), (-30, 40), (-30, 50), (-30, 60), (-30, 70), (-30, 80), (-30, 90), (-20, -20), (-20, -10), (-20, 0), (-20, 10), (-20, 20), (-20, 30), (-20, 40), (-20, 50), (-20, 60), (-20, 70), (-20, 80), (-20, 90), (-20, 100), (-10, -30), (-10, -20), (-10, -10), (-10, 0), (-10, 10), (-10, 20), (-10, 30), (-10, 40), (-10, 50), (-10, 60), (-10, 70), (-10, 80), (-10, 90), (-10, 100), (-10, 110), (0, -40), (0, -30), (0, -20), (0, -10), (0, 0), (0, 10), (0, 20), (0, 30), (0, 40), (0, 50), (0, 60), (0, 70), (0, 80), (0, 90), (0, 100), (0, 110), (0, 120), (10, -50), (10, -40), (10, -30), (10, -20), (10, -10), (10, 0), (10, 10), (10, 20), (10, 30), (10, 40), (10, 50), (10, 60), (10, 70), (10, 80), (10, 90), (10, 100), (10, 110), (10, 120), (20, -50), (20, -40), (20, -30), (20, -20), (20, -10), (20, 0), (20, 10), (20, 20), (20, 30), (20, 40), (20, 50), (20, 60), (20, 70), (20, 80), (20, 90), (20, 100), (20, 110), (20, 120), (20, 130), (30, -60), (30, -50), (30, -40), (30, -30), (30, -20), (30, -10), (30, 0), (30, 10), (30, 20), (30, 30), (30, 40), (30, 50), (30, 60), (30, 70), (30, 80), (30, 90), (30, 100), (30, 110), (30, 120), (30, 130), (40, -60), (40, -50), (40, -40), (40, -30), (40, -20), (40, -10), (40, 0), (40, 10), (40, 20), (40, 30), (40, 40), (40, 50), (40, 60), (40, 70), (40, 80), (40, 90), (40, 100), (40, 110), (40, 120), (40, 130), (40, 140), (50, -60), (50, -50), (50, -40), (50, -30), (50, -20), (50, -10), (50, 0), (50, 10), (50, 20), (50, 30), (50, 40), (50, 50), (50, 60), (50, 70), (50, 80), (50, 90), (50, 100), (50, 110), (50, 120), (50, 130), (50, 140), (60, -60), (60, -50), (60, -40), (60, -30), (60, -20), (60, -10), (60, 0), (60, 10), (60, 20), (60, 30), (60, 40), (60, 50), (60, 60), (60, 70), (60, 80), (60, 90), (60, 100), (60, 110), (60, 120), (60, 130), (60, 140), (70, -60), (70, -50), (70, -40), (70, -30), (70, -20), (70, -10), (70, 0), (70, 10), (70, 20), (70, 30), (70, 40), (70, 50), (70, 60), (70, 70), (70, 80), (70, 90), (70, 100), (70, 110), (70, 120), (70, 130), (70, 140), (80, -60), (80, -50), (80, -40), (80, -30), (80, -20), (80, -10), (80, 0), (80, 10), (80, 20), (80, 30), (80, 40), (80, 50), (80, 60), (80, 70), (80, 80), (80, 90), (80, 100), (80, 110), (80, 120), (80, 130), (80, 140), (90, -50), (90, -40), (90, -30), (90, -20), (90, -10), (90, 0), (90, 10), (90, 20), (90, 30), (90, 40), (90, 50), (90, 60), (90, 70), (90, 80), (90, 90), (90, 100), (90, 110), (90, 120), (90, 130), (100, -50), (100, -40), (100, -30), (100, -20), (100, -10), (100, 0), (100, 10), (100, 20), (100, 30), (100, 40), (100, 50), (100, 60), (100, 70), (100, 80), (100, 90), (100, 100), (100, 110), (100, 120), (100, 130), (110, -40), (110, -30), (110, -20), (110, -10), (110, 0), (110, 10), (110, 20), (110, 30), (110, 40), (110, 50), (110, 60), (110, 70), (110, 80), (110, 90), (110, 100), (110, 110), (110, 120), (110, 130), (120, -40), (120, -30), (120, -20), (120, -10), (120, 0), (120, 10), (120, 20), (120, 30), (120, 40), (120, 50), (120, 60), (120, 70), (120, 80), (120, 90), (120, 100), (120, 110), (120, 120), (130, -30), (130, -20), (130, -10), (130, 0), (130, 10), (130, 20), (130, 30), (130, 40), (130, 50), (130, 60), (130, 70), (130, 80), (130, 90), (130, 100), (130, 110), (140, -20), (140, -10), (140, 0), (140, 10), (140, 20), (140, 30), (140, 40), (140, 50), (140, 60), (140, 70), (140, 80), (140, 90), (140, 100), (150, 0), (150, 10), (150, 20), (150, 30), (150, 40), (150, 50), (150, 60), (150, 70), (150, 80)]\n",
      "[0.0010272813839087899, 0.0010731497560238698, 0.001091320740687877, 0.001080779791435198, 0.0010424585391064344, 0.0010192704738400775, 0.0011552445071180448, 0.001270539272368318, 0.0013573711296848054, 0.0014100334625200609, 0.0014253891881311372, 0.0014030298615317928, 0.0013451490237220013, 0.0012562049702333328, 0.0011424397640503988, 0.0010113301538827112, 0.0011349860281273732, 0.0013239257839749996, 0.001495970344216125, 0.0016392549281208564, 0.0017438649128279928, 0.00180292506761729, 0.0018132047113246263, 0.0017752085642782074, 0.0016928344514809705, 0.0015727366506309032, 0.0014235329451101697, 0.0012550036923875245, 0.0010771180504660118, 0.0011896084268915121, 0.0014322644281206313, 0.001667855365091333, 0.0018803962721446851, 0.0020547758524914425, 0.0021786413279358623, 0.002243826899155104, 0.0022470933733325024, 0.002190131008597631, 0.002078945255286458, 0.0019228428740827978, 0.0017332479544122105, 0.001522591812930559, 0.0013030641555094526, 0.0010859544780974964, 0.0011686952459237817, 0.0014558240152426393, 0.0017517666285060643, 0.0020378995750836406, 0.0022942935335956817, 0.002502248026748422, 0.0026467109079782282, 0.002718097331535332, 0.002713189953630568, 0.002635046575236336, 0.0024920747096435986, 0.0022965798776679374, 0.0020631251631617257, 0.0018070626341162126, 0.0015429684144721407, 0.0012839588825972143, 0.0010410575866918411, 0.0010726361567223773, 0.0013850328819523792, 0.0017257969123208074, 0.0020766188316996663, 0.002415018147835471, 0.0027169559321730758, 0.002959953789518086, 0.0031260843231080736, 0.0032042185883698424, 0.0031911153452348845, 0.003091243328817119, 0.002915532249469049, 0.002679452289506672, 0.002400877322457298, 0.0020982245949291473, 0.0017885410100359962, 0.0014868013499981392, 0.0012052390170292867, 0.001229734596771176, 0.001589323430890032, 0.0019817972760158655, 0.002385890962556666, 0.0027754139420341295, 0.00312231068171321, 0.003400348028043291, 0.003588677102461201, 0.003674525755611651, 0.0036545037176093083, 0.0035343676453790766, 0.0033274692444627395, 0.0030523740927885677, 0.0027302243182567914, 0.0023824768314267275, 0.002028622911104732, 0.0016854684040108446, 0.0013664158789992394, 0.001080817167173125, 0.0010168167490169156, 0.0013646959008510795, 0.001765941774085952, 0.0022044971900612234, 0.002656560838361663, 0.0030926703268028405, 0.003481143329003276, 0.0037922710120529444, 0.004002409017350022, 0.004097102683523417, 0.004072627504214389, 0.003935740643384951, 0.0037018808772763664, 0.003392378161005821, 0.0030313533025504297, 0.0026430741248909036, 0.0022493117155413756, 0.0018685953453441142, 0.0015154277683693314, 0.0011998368639005786, 0.0010906033774946127, 0.0014660421634194896, 0.001899978934765195, 0.0023752501200662262, 0.002866166132356627, 0.0033407139026030356, 0.0037642757130562886, 0.004104221113528197, 0.004334439903056732, 0.004438854324606051, 0.004413200219884241, 0.004264817554488563, 0.004010685739644747, 0.003674316150340757, 0.0032822684772292696, 0.00286117556048491, 0.0024347660943008814, 0.0020230821805853963, 0.0016416083558318108, 0.0013010121996042557, 0.001007977566769058, 0.0011321098460634334, 0.0015245420201100862, 0.001979253044055763, 0.0024785768755071925, 0.0029957605005515274, 0.003497196563030591, 0.00394628994859493, 0.004308303088763855, 0.004555203213367457, 0.004669481304949802, 0.004646159544646631, 0.004492672935922495, 0.004226841937175413, 0.003873571790564344, 0.0034611015278220854, 0.003017777766483611, 0.0025688024644995085, 0.002135400452163823, 0.0017338477167176847, 0.0013753706736056933, 0.0010668945903243834, 0.0011373627846360294, 0.001534562893951577, 0.0019960891279407074, 0.002504420346588275, 0.003032663895618032, 0.0035467270606294593, 0.004009188003447453, 0.004384225257767052, 0.004642620989996111, 0.004765776133884741, 0.00474790668251961, 0.00459605827289214, 0.004328122666785561, 0.003969485141682068, 0.003549147017498118, 0.0030963522229565618, 0.0026371459268073145, 0.002193483852772172, 0.0017821561796059469, 0.0014148020495548816, 0.0010985310074840822, 0.0011057616558322744, 0.0014949455623922182, 0.0019485195867082624, 0.0024497296389317342, 0.002972473426495742, 0.0034833281347144227, 0.0039452841939910865, 0.004322582792661318, 0.004585701275669755, 0.004715432463021755, 0.004705209910047409, 0.0045612792107354635, 0.004300855282432782, 0.003948859739158658, 0.003534067581983646, 0.003085704286435084, 0.0026299223916376544, 0.002188853263928017, 0.0017794377535770458, 0.0014135080771875128, 0.0010982533974095766, 0.0010401879429560634, 0.001409218406675032, 0.0018406520882359991, 0.0023190361711178187, 0.0028199025262013186, 0.003311586392159063, 0.0037586979736714396, 0.004126702845371137, 0.0043867175058021485, 0.0045195122421831406, 0.004517887934773722, 0.004387005628575004, 0.004142760240553677, 0.003808734416737916, 0.0034125135563247587, 0.0029823733854069225, 0.0025437929792241597, 0.0021184609706061557, 0.0017230396983275915, 0.0013692704133737737, 0.0010642710552799257, 0.0012850979785399318, 0.0016821119894878235, 0.002123874776002746, 0.0025882324586918295, 0.0030461965675405325, 0.0034650565575847, 0.0038125650314436736, 0.004061390597258617, 0.004192909585622165, 0.004199547917859853, 0.004085248796895266, 0.003864109906792435, 0.003557652171269465, 0.0031914261561529424, 0.0027919002691506544, 0.002383127829310903, 0.001985730828866139, 0.0016156372167887541, 0.0012841752986108164, 0.0011333682756932944, 0.0014866559546554236, 0.0018811399846344026, 0.0022974456550215777, 0.0027099325596526376, 0.003089386060355229, 0.003406707842793539, 0.0036369065488064314, 0.0037625634661671518, 0.0037760543711271794, 0.003680118214899992, 0.0034867745493542546, 0.0032149701295960307, 0.0028875678360242014, 0.0025285253823209442, 0.002159820688029665, 0.0018004445262110673, 0.0014651579456827278, 0.001164554683829797, 0.0012702249361768066, 0.0016106920362983722, 0.001971393052745385, 0.0023304208864015543, 0.0026625718210970175, 0.0029424861026461904, 0.003148099120937164, 0.003263696748834186, 0.003281943701826243, 0.003204506627480851, 0.0030412413651553635, 0.0028082433340970592, 0.0025252726851065087, 0.002213285903827185, 0.0018916999242297602, 0.0015774284257744227, 0.001283706734661826, 0.0010201139455777168, 0.0010488132772536005, 0.0013326785892155248, 0.0016345499108819201, 0.0019363515360782753, 0.0022170880937502826, 0.0024554233988312224, 0.002632559986943348, 0.002734836226735169, 0.0027555073503245007, 0.0026953752054149523, 0.002562214592662991, 0.0023692200854184458, 0.0021328836763532776, 0.0018709110478470074, 0.0015998744418566492, 0.0013343253504817143, 0.0010857277658160203, 0.0010650752321712399, 0.0013089791633614095, 0.001553860798523004, 0.0017828382871868014, 0.001978589904777992, 0.002125671445956336, 0.002212644100893984, 0.002233578234310677, 0.0021886481611192994, 0.002083754816873049, 0.0019293357710224155, 0.0017386789158446681, 0.0015262256524970533, 0.0013056302722814278, 0.0010889749752930177, 0.0010120322373960175, 0.0012037336654424219, 0.0013838693842845042, 0.0015388752085500404, 0.0016565189983247098, 0.0017275806397219029, 0.001747072792407573, 0.0017147700089923572, 0.0016349811274248031, 0.0015156725405707796, 0.0013671767114254332, 0.0012008596147964406, 0.001027574887590965, 0.0010364596166873715, 0.0011547458666443507, 0.0012453554992481847, 0.0013011363098688747, 0.0013180724077294977, 0.0012957299212764532, 0.0012371351278736224, 0.0011481534585608031, 0.0010365362199347536]\n"
     ]
    }
   ],
   "source": [
    "#Color stats\n",
    "\n",
    "# need dataset express like that: list of images (as tensor) with dim H x W x 2 ...\n",
    "images = [np.random.rand(3,3,2) * 100]\n",
    "print(images)\n",
    "\n",
    "# Initiate the proba distribution of ab pairs in the images dataset (discretised).\n",
    "gridSize = 10\n",
    "colorProbabilities = np.zeros((1000 // gridSize, 400 // gridSize))\n",
    "\n",
    "# Compute the proba distribution of the ab pairs in the images dataset (discretised).\n",
    "nbOfAnalysedPixels = 0\n",
    "for image in images:\n",
    "    for h in range (image.shape[0]):\n",
    "        for w in range (image.shape[1]):\n",
    "            (a,b) = getDiscretisedColor(image[h][w][0],image[h][w][1],gridSize)\n",
    "            (i,j) = getMatrixIndex(a,b,gridSize)\n",
    "            colorProbabilities[i][j] += 1\n",
    "            nbOfAnalysedPixels += 1\n",
    "colorProbabilities = colorProbabilities / nbOfAnalysedPixels\n",
    "\n",
    "# Smooth the proba distribution of the ab pairs in the images dataset.\n",
    "sigma = 5 # gaussian kernel parameter\n",
    "colorProbabilities = gaussian_filter(colorProbabilities, sigma=sigma) # is it ok ? add a lot of value in gamut... take proba treshold ?\n",
    "\n",
    "# + display distribution in 2d plot ? like in paper (here, very simple)\n",
    "plt.imshow(colorProbabilities, interpolation='none')\n",
    "plt.show()\n",
    "\n",
    "# Get the vector of proba of ab pairs that are \"in gamut\"\n",
    "inGamutColors = []\n",
    "inGamutColorsProbas = []\n",
    "for i in range (colorProbabilities.shape[0]):\n",
    "    for j in range (colorProbabilities.shape[1]):\n",
    "        currentColorPorba = colorProbabilities[i][j]\n",
    "        if currentColorPorba >= 0.001: # put a treshold ? if we use smoothing \n",
    "            (a,b) = getColorValue(i,j,gridSize)\n",
    "            inGamutColors.append((a,b))\n",
    "            inGamutColorsProbas.append(currentColorPorba)\n",
    "        \n",
    "Q = len(inGamutColors) \n",
    "p_smooth = torch.tensor(inGamutColorsProbas)\n",
    "print(Q)\n",
    "print(inGamutColors)\n",
    "print(inGamutColorsProbas)\n",
    "Q = 1 # to remove\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define pixel weight vector (class rebalancing)\n",
    "\n",
    "# Set the parameters (from paper, need empirical value).\n",
    "lambda_uniform = 1/2 \n",
    "\n",
    "# Compute a smooth version of the empirical pixel color distribution.\n",
    "#p_smooth = p # how to do that ? gaussian kernel ? done at previous cell...\n",
    "\n",
    "# Compute the weight vector.\n",
    "pixelsWeights = torch.reciprocal((1 - lambda_uniform) * p_smooth + lambda_uniform / Q)\n",
    "\n",
    "# Normalise the weight vector according to p_smooth (E[W] = 1).\n",
    "E_W = torch.sum(p_smooth * pixelsWeights)\n",
    "scale_factor = 1 / E_W\n",
    "pixelsWeights = scale_factor * pixelsWeights "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define and instantiate Convolutional NN consistent with the description of the paper. Shown in table 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/diomede/Documents/ULG/DL-image-colorization/image-colorization.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/diomede/Documents/ULG/DL-image-colorization/image-colorization.ipynb#ch0000009?line=0'>1</a>\u001b[0m \u001b[39m#NN def\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/diomede/Documents/ULG/DL-image-colorization/image-colorization.ipynb#ch0000009?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcv2\u001b[39;00m \u001b[39mimport\u001b[39;00m dilate\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/diomede/Documents/ULG/DL-image-colorization/image-colorization.ipynb#ch0000009?line=5'>6</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mColorizationCNN\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/diomede/Documents/ULG/DL-image-colorization/image-colorization.ipynb#ch0000009?line=6'>7</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "#NN def\n",
    "\n",
    "from cv2 import dilate\n",
    "\n",
    "\n",
    "class ColorizationCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.l_cent = 50.\n",
    "        self.l_norm = 100.\n",
    "        self.ab_norm = 110.\n",
    "        \n",
    "        channels_block_1 = 64\n",
    "        channels_block_2 = 128\n",
    "        channels_block_3 = 256\n",
    "        channels_block_4 = 512\n",
    "        channels_block_5 = 512 #dilated\n",
    "        channels_block_6 = 512 #dilated\n",
    "        channels_block_7 = 512 \n",
    "        channels_block_8 = 128 # transpose convolution necessary\n",
    "\n",
    "        nb_colour_bins = 313\n",
    "        # first conv block : 2 convs. from luminosity image to 64 features map from 3x3 kernels. 50% downsampling and normalization at the end.\n",
    "        self.convBlock1 = nn.Sequential(nn.Conv2d(1,channels_block_1,(3,3), padding =1), \n",
    "                nn.ReLU(True), #inplace for memory efficiency can be used as no skip connections are used.\n",
    "                nn.Conv2d(channels_block_1,channels_block_1,(3,3), padding =1,stride=2), #50% downsampling achieved with a 2 stride. \n",
    "                nn.Relu(True),\n",
    "                nn.BatchNorm2d(channels_block_1) #normalization over the 64 channels created\n",
    "        )\n",
    "\n",
    "        # second conv block. 2 covs. from 64 features to 128 features map from 3x3 kernels. 50% downsampling and normalization at the end.\n",
    "        self.convBlock2 = nn.Sequential(nn.Conv2d(64,channels_block_2,(3,3), padding =1,), \n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(channels_block_2,channels_block_2,(3,3), padding =1,stride=2), #50% downsampling achieved with a 2 stride. \n",
    "                nn.ReLU(True),\n",
    "                nn.BatchNorm2d(channels_block_2)\n",
    "        )\n",
    "\n",
    "        # third conv block. 3 convs. from 64 to 128 features map from 3x3 kernels. 50% downsampling and normalization at the end.\n",
    "        self.convBlock3 = nn.Sequential(nn.Conv2d(channels_block_2,channels_block_3,(3,3), padding =1,), \n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(channels_block_3,channels_block_3,(3,3), padding =1),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(channels_block_3,channels_block_3,(3,3), padding =1,stride=2), #50% downsampling achieved with a 2 stride. \n",
    "                nn.ReLU(True),\n",
    "                nn.BatchNorm2d(channels_block_3)\n",
    "        )\n",
    "\n",
    "        # fourth conv block. 3 convs. from 256 to 512 features map from 3x3 kernels. 50% downsampling and normalization at the end.\n",
    "        self.convBlock4 = nn.Sequential(nn.Conv2d(channels_block_3,channels_block_4,(3,3), padding =1,),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(channels_block_4,channels_block_4,(3,3), padding =1),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(channels_block_4,channels_block_4,(3,3), padding =1), #50% downsampling achieved with a 2 stride. \n",
    "                nn.ReLU(True),\n",
    "                nn.BatchNorm2d(channels_block_4)\n",
    "        )\n",
    "        \n",
    "        #fifth conv block. 3 convs. no change in nb feature maps. 3x3 kernels with 2 dilation and 2 padding to not downscale. normalization at the end.\n",
    "\n",
    "        self.convBlock5 = nn.Sequential(nn.Conv2d(channels_block_4,channels_block_5,(3,3),dilation=2,padding=2),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(channels_block_5,channels_block_5,(3,3),dilation=2,padding=2),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(channels_block_5,channels_block_5,(3,3),dilation=2,padding=2), #50% downsampling achieved with a 2 stride. \n",
    "                nn.ReLU(True),\n",
    "                nn.BatchNorm2d(channels_block_5)\n",
    "        )\n",
    "\n",
    "        #sixth conv block. same as 5\n",
    "        self.convBlock6 = nn.Sequential(nn.Conv2d(channels_block_5,channels_block_6,(3,3),dilation=2,padding=2),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(channels_block_6,channels_block_6,(3,3),dilation=2,padding=2),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(channels_block_6,channels_block_6,(3,3),dilation=2,padding=2), #50% downsampling achieved with a 2 stride. \n",
    "                nn.ReLU(True),\n",
    "                nn.BatchNorm2d(channels_block_6)\n",
    "        )\n",
    "\n",
    "        #seventh conv block : 3 convs with 3x3 kernels.\n",
    "        self.convBlock7 = nn.Sequential(nn.Conv2d(channels_block_6,channels_block_7,(3,3),padding =1,),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(channels_block_7,channels_block_7,(3,3),padding =1,),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(channels_block_7,channels_block_7,(3,3),padding =1,), #50% downsampling achieved with a 2 stride. \n",
    "                nn.ReLU(True),\n",
    "                nn.BatchNorm2d(channels_block_7)\n",
    "        )\n",
    "\n",
    "        #eighth conv block : 1 inverse conv to upsample then 2 convs with 3x3 kernels default parameters. Final convolution with 1x1 for classification into a colour bin\n",
    "        self.convBlock8 = nn.Sequential(nn.ConvTranspose2d(channels_block_7,channels_block_8,(4,4),stride = 2, padding =1),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(channels_block_8,channels_block_8,(3,3),padding = 1),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(channels_block_8,channels_block_8,(3,3),padding = 1), #50% downsampling achieved with a 2 stride. \n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(channels_block_8,nb_colour_bins,kernel_size=1) #1x1 kernel for classification in each colour bin (value will be soft maxed for probability)\n",
    "        )\n",
    "\n",
    "        self.outputLayer = nn.Conv2d(nb_colour_bins,2,kernel_size=1,dilation=1, bias = False) # 1x1 kernel to get 2 channel values of a and b respectively\n",
    "    \n",
    "    def forward(self, luminosity_image):\n",
    "        h1 = self.convBlock1((luminosity_image-self.l_cent)/self.l_norm) #normalize luminosity to be on scale of 0 to 100\n",
    "        h2 = self.convBlock2(h1)\n",
    "        h3 = self.convBlock3(h2)\n",
    "        h4 = self.convBlock4(h3)\n",
    "        h5 = self.convBlock5(h4)\n",
    "        h6 = self.convBlock6(h5)\n",
    "        h7 = self.convBlock7(h6)\n",
    "        h8 = self.convBlock8(h7)\n",
    "\n",
    "        colour_bin_proba = (nn.Softmax(dim=1))(h8)\n",
    "        output = self.outputLayer(colour_bin_proba)\n",
    "        upscaled_output = (nn.Upsample(scale_factor=4, mode='bilinear'))(output) # bilinear upscale to agree with input image size \n",
    "\n",
    "        return upscaled_output * self.ab_norm # denormalize to cover whole ab value range\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADAM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert a true image Y to pixels color distributions Z (soft encoding)\n",
    "\n",
    "# to implement...\n",
    "def getColorDistribution(Y):\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the pixel color distributions in Z[HxWxQ] to true picture estimate Y[HxWxQ] (point estimate)\n",
    "\n",
    "# No need to be very efficient because only use when we predict (after training) ? difficult with tensor operation\n",
    "\n",
    "def getPictureEstimate(Z,T):\n",
    "    # Initiate a tensor to store the image estimated from Z.\n",
    "    Y_estimate = torch.zeros(Z.shape[0], Z.shape[1], 2)\n",
    "    \n",
    "    # Estimate the Lab color for each pixel of the image.\n",
    "    for h in range (Y_estimate.shape[0]):\n",
    "        for w in range (Y_estimate.shape[1]):\n",
    "            # Re-ajust the temperture of the current distribution.\n",
    "            reajustedDistribution = torch.exp(torch.log10(Z[h][w]) / T)  / torch.sum(torch.exp(torch.log10(Z[h][w]) / T), 2) # check again...\n",
    "\n",
    "            # Compute the anneled-mean of the current distribution. \n",
    "            a, b = 0, 0\n",
    "            for q in range (Z.shape[2]):\n",
    "                a += reajustedDistribution[q] * inGamutColors[q][0]\n",
    "                b += reajustedDistribution[q] * inGamutColors[q][1]\n",
    "            \n",
    "            # Estimate the Lab color for the current pixel.\n",
    "            Y_estimate[h][w][0] = a\n",
    "            Y_estimate[h][w][1] = b\n",
    "    \n",
    "    # Return the estimated picture.\n",
    "    return Y_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v(Z_hw) weight in paper (section 2)\n",
    "\n",
    "def getPixelsWeights(Z):\n",
    "    W = torch.argmax(Z, dim=2)\n",
    "    for i in range(W.size(dim=0)):\n",
    "        for j in range(W.size(dim=1)):\n",
    "            W[i,j] = pixelsWeights(W[i,j])\n",
    "\n",
    "    return W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function \n",
    "\n",
    "def multinomialCrossEntropyLoss(Z_estimate, Z):\n",
    "    W = getPixelsWeights(Z)\n",
    "    L = - sum(W * torch.sum(Z * torch.log10(Z_estimate), dim=2))\n",
    "\n",
    "    return L\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main training loop and additional function TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('deep_learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "464699cf85eadd29f123288e0bb2f83c79f3ce7e20f2c87b57abaa5e0b3edf52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
