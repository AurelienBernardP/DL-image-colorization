{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image colorization project :\n",
    "We train a CNN to take in greyscale images of ... and output their colorful and plausible colorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports required :\n",
    "\n",
    "torch\n",
    "\n",
    "skimage ? only used for rgb - lab parsing. Maybe we can do that ourselves to make some differentiation with the original project?\n",
    "\n",
    "numpy\n",
    "\n",
    "matplotlib\n",
    "\n",
    "PIL ? only used to open images to rgb. We can use another library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from skimage import color\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data, augment it, transform it to LAB compute stats on colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LAB transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[36.12643395, 74.7237478 ],\n",
      "        [68.27200645, 57.15841288],\n",
      "        [30.58742431,  7.87857414]],\n",
      "\n",
      "       [[78.91004163,  6.35860304],\n",
      "        [30.04847888, 22.25479969],\n",
      "        [86.93046478, 58.48898759]],\n",
      "\n",
      "       [[89.91140275, 79.43439784],\n",
      "        [21.15828357, 92.47416693],\n",
      "        [81.51745681, 44.85176954]]])]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "[(20, 90), (30, 10), (30, 20), (40, 70), (70, 60), (80, 10), (80, 40), (90, 60), (90, 80)]\n",
      "[0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111]\n"
     ]
    }
   ],
   "source": [
    "#Color stats\n",
    "\n",
    "# need dataset express like that: list of images (as tensor) with dim H x W x 2 ...\n",
    "images = [np.random.rand(3,3,2) * 100]\n",
    "print(images)\n",
    "\n",
    "def getDiscretisedColor(a,b,gridSize):\n",
    "    a = np.round(a/gridSize) * gridSize\n",
    "    b = np.round(b/gridSize) * gridSize\n",
    "    return (a,b)\n",
    "\n",
    "def getMatrixIndex(a,b,gridSize):\n",
    "    i = (a + 500) / gridSize\n",
    "    j = (b + 200) / gridSize\n",
    "    return (int(i),int(j))\n",
    "\n",
    "def getColorValue(i,j,gridSize):\n",
    "    a = i * gridSize - 500\n",
    "    b = j * gridSize - 200\n",
    "    return (a,b)\n",
    "\n",
    "# Initiate the proba distribution of ab pairs in the images dataset (discretised).\n",
    "gridSize = 10\n",
    "colorProbabilities = np.zeros((1000 // gridSize, 400 // gridSize))\n",
    "\n",
    "# Compute the proba distribution of the ab pairs in the images dataset (discretised).\n",
    "nbOfAnalysedPixels = 0\n",
    "for image in images:\n",
    "    for h in range (image.shape[0]):\n",
    "        for w in range (image.shape[1]):\n",
    "            (a,b) = getDiscretisedColor(image[h][w][0],image[h][w][1],gridSize)\n",
    "            (i,j) = getMatrixIndex(a,b,gridSize)\n",
    "            colorProbabilities[i][j] += 1\n",
    "            nbOfAnalysedPixels += 1\n",
    "colorProbabilities = colorProbabilities / nbOfAnalysedPixels\n",
    "\n",
    "# + display distribution in 2d plot ? like in paper (here, very simple)\n",
    "plt.imshow(colorProbabilities, interpolation='none')\n",
    "plt.show()\n",
    "\n",
    "# Get the vector of proba of ab pairs that are \"in gamut\"\n",
    "inGamutColors = []\n",
    "inGamutColorsProbas = []\n",
    "for i in range (colorProbabilities.shape[0]):\n",
    "    for j in range (colorProbabilities.shape[1]):\n",
    "        currentColorPorba = colorProbabilities[i][j]\n",
    "        if currentColorPorba != 0:\n",
    "            (a,b) = getColorValue(i,j,gridSize)\n",
    "            inGamutColors.append((a,b))\n",
    "            inGamutColorsProbas.append(currentColorPorba)\n",
    "        \n",
    "Q = len(inGamutColors) \n",
    "p = torch.tensor(inGamutColorsProbas)\n",
    "print(Q)\n",
    "print(inGamutColors)\n",
    "print(inGamutColorsProbas)\n",
    "Q = 1 # to remove\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define pixel weight vector (class rebalancing)\n",
    "\n",
    "# Set the parameters (from paper, need empirical value).\n",
    "lambda_uniform = 1/2 \n",
    "sigma = 5 # gaussian kernel parameter\n",
    "\n",
    "# Compute a smooth version of the empirical pixel color distribution.\n",
    "p_smooth = p # how to do that ? gaussian kernel ? to do\n",
    "\n",
    "# Compute the weight vector.\n",
    "pixelsWeights = torch.reciprocal((1 - lambda_uniform) * p_smooth + lambda_uniform / Q)\n",
    "\n",
    "# Normalise the weight vector according to p_smooth (E[W] = 1).\n",
    "E_W = torch.sum(p_smooth * pixelsWeights)\n",
    "scale_factor = 1 / E_W\n",
    "pixelsWeights = scale_factor * pixelsWeights "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define and instantiate Convolutional NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NN def"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert a true pixel Y to a distribution Z (soft encoding)\n",
    "\n",
    "# to implement...\n",
    "def getColorDistribution(Y):\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert a distribution Z to a true pixel Y (point estimate)\n",
    "\n",
    "# to implement...\n",
    "def getColorEstimate(Z):\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v(Z_hw) weight in paper (section 2)\n",
    "\n",
    "def getPixelsWeights(Z):\n",
    "    W = torch.argmax(Z, dim=2)\n",
    "    for i in range(W.size(dim=0)):\n",
    "        for j in range(W.size(dim=1)):\n",
    "            W[i,j] = pixelsWeights(W[i,j])\n",
    "\n",
    "    return W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function \n",
    "\n",
    "def multinomialCrossEntropyLoss(Z_estimate, Z):\n",
    "    W = getPixelsWeights(Z)\n",
    "    L = - sum(W * torch.sum(Z * torch.log10(Z_estimate), dim=2))\n",
    "\n",
    "    return L\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main training loop and additional function TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('deep_learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "464699cf85eadd29f123288e0bb2f83c79f3ce7e20f2c87b57abaa5e0b3edf52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
