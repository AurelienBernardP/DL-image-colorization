{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image colorization project :\n",
    "We train a CNN to take in greyscale images of ... and output their colorful and plausible colorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports required :\n",
    "\n",
    "torch\n",
    "\n",
    "skimage ? only used for rgb - lab parsing. Maybe we can do that ourselves to make some differentiation with the original project?\n",
    "\n",
    "numpy\n",
    "\n",
    "matplotlib\n",
    "\n",
    "PIL ? only used to open images to rgb. We can use another library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "#from skimage import color\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data, augment it, transform it to LAB compute stats on colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LAB transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[36.12643395, 74.7237478 ],\n",
      "        [68.27200645, 57.15841288],\n",
      "        [30.58742431,  7.87857414]],\n",
      "\n",
      "       [[78.91004163,  6.35860304],\n",
      "        [30.04847888, 22.25479969],\n",
      "        [86.93046478, 58.48898759]],\n",
      "\n",
      "       [[89.91140275, 79.43439784],\n",
      "        [21.15828357, 92.47416693],\n",
      "        [81.51745681, 44.85176954]]])]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "[(20, 90), (30, 10), (30, 20), (40, 70), (70, 60), (80, 10), (80, 40), (90, 60), (90, 80)]\n",
      "[0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111]\n"
     ]
    }
   ],
   "source": [
    "#Color stats\n",
    "\n",
    "# need dataset express like that: list of images (as tensor) with dim H x W x 2 ...\n",
    "images = [np.random.rand(3,3,2) * 100]\n",
    "print(images)\n",
    "\n",
    "def getDiscretisedColor(a,b,gridSize):\n",
    "    a = np.round(a/gridSize) * gridSize\n",
    "    b = np.round(b/gridSize) * gridSize\n",
    "    return (a,b)\n",
    "\n",
    "def getMatrixIndex(a,b,gridSize):\n",
    "    i = (a + 500) / gridSize\n",
    "    j = (b + 200) / gridSize\n",
    "    return (int(i),int(j))\n",
    "\n",
    "def getColorValue(i,j,gridSize):\n",
    "    a = i * gridSize - 500\n",
    "    b = j * gridSize - 200\n",
    "    return (a,b)\n",
    "\n",
    "# Initiate the proba distribution of ab pairs in the images dataset (discretised).\n",
    "gridSize = 10\n",
    "colorProbabilities = np.zeros((1000 // gridSize, 400 // gridSize))\n",
    "\n",
    "# Compute the proba distribution of the ab pairs in the images dataset (discretised).\n",
    "nbOfAnalysedPixels = 0\n",
    "for image in images:\n",
    "    for h in range (image.shape[0]):\n",
    "        for w in range (image.shape[1]):\n",
    "            (a,b) = getDiscretisedColor(image[h][w][0],image[h][w][1],gridSize)\n",
    "            (i,j) = getMatrixIndex(a,b,gridSize)\n",
    "            colorProbabilities[i][j] += 1\n",
    "            nbOfAnalysedPixels += 1\n",
    "colorProbabilities = colorProbabilities / nbOfAnalysedPixels\n",
    "\n",
    "# + display distribution in 2d plot ? like in paper (here, very simple)\n",
    "plt.imshow(colorProbabilities, interpolation='none')\n",
    "plt.show()\n",
    "\n",
    "# Get the vector of proba of ab pairs that are \"in gamut\"\n",
    "inGamutColors = []\n",
    "inGamutColorsProbas = []\n",
    "for i in range (colorProbabilities.shape[0]):\n",
    "    for j in range (colorProbabilities.shape[1]):\n",
    "        currentColorPorba = colorProbabilities[i][j]\n",
    "        if currentColorPorba != 0:\n",
    "            (a,b) = getColorValue(i,j,gridSize)\n",
    "            inGamutColors.append((a,b))\n",
    "            inGamutColorsProbas.append(currentColorPorba)\n",
    "        \n",
    "Q = len(inGamutColors) \n",
    "p = torch.tensor(inGamutColorsProbas)\n",
    "print(Q)\n",
    "print(inGamutColors)\n",
    "print(inGamutColorsProbas)\n",
    "Q = 1 # to remove\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define pixel weight vector (class rebalancing)\n",
    "\n",
    "# Set the parameters (from paper, need empirical value).\n",
    "lambda_uniform = 1/2 \n",
    "sigma = 5 # gaussian kernel parameter\n",
    "\n",
    "# Compute a smooth version of the empirical pixel color distribution.\n",
    "p_smooth = p # how to do that ? gaussian kernel ? to do\n",
    "\n",
    "# Compute the weight vector.\n",
    "pixelsWeights = torch.reciprocal((1 - lambda_uniform) * p_smooth + lambda_uniform / Q)\n",
    "\n",
    "# Normalise the weight vector according to p_smooth (E[W] = 1).\n",
    "E_W = torch.sum(p_smooth * pixelsWeights)\n",
    "scale_factor = 1 / E_W\n",
    "pixelsWeights = scale_factor * pixelsWeights "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define and instantiate Convolutional NN consistent with the description of the paper. Shown in table 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NN def\n",
    "\n",
    "class ColorizationCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.l_cent = 50.\n",
    "        self.l_norm = 100.\n",
    "        self.ab_norm = 110.\n",
    "        \n",
    "        channels_block_1 = 64\n",
    "        channels_block_2 = 128\n",
    "        channels_block_3 = 256\n",
    "        channels_block_4 = 512\n",
    "        channels_block_5 = 512 #dilated\n",
    "        channels_block_6 = 512 #dilated\n",
    "        channels_block_7 = 512 \n",
    "        channels_block_8 = 128 # transpose convolution necessary\n",
    "\n",
    "        nb_colour_bins = 313\n",
    "        # first conv block : 2 convs. from luminosity image to 64 features map from 3x3 kernels. 50% downsampling and normalization at the end.\n",
    "        self.convBlock1 = nn.Sequential(nn.Conv2d(1,channels_block_1,(3,3), padding =1), \n",
    "                nn.ReLU(True), #inplace for memory efficiency can be used as no skip connections are used.\n",
    "                nn.Conv2d(channels_block_1,channels_block_1,(3,3), padding =1,stride=2), #50% downsampling achieved with a 2 stride. \n",
    "                nn.ReLU(True),\n",
    "                nn.BatchNorm2d(channels_block_1) #normalization over the 64 channels created\n",
    "        )\n",
    "\n",
    "        # second conv block. 2 covs. from 64 features to 128 features map from 3x3 kernels. 50% downsampling and normalization at the end.\n",
    "        self.convBlock2 = nn.Sequential(nn.Conv2d(64,channels_block_2,(3,3), padding =1,), \n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(channels_block_2,channels_block_2,(3,3), padding =1,stride=2), #50% downsampling achieved with a 2 stride. \n",
    "                nn.ReLU(True),\n",
    "                nn.BatchNorm2d(channels_block_2)\n",
    "        )\n",
    "\n",
    "        # third conv block. 3 convs. from 64 to 128 features map from 3x3 kernels. 50% downsampling and normalization at the end.\n",
    "        self.convBlock3 = nn.Sequential(nn.Conv2d(channels_block_2,channels_block_3,(3,3), padding = 1,), \n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(channels_block_3,channels_block_3,(3,3), padding =1),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(channels_block_3,channels_block_3,(3,3), padding =1,stride=2), #50% downsampling achieved with a 2 stride. \n",
    "                nn.ReLU(True),\n",
    "                nn.BatchNorm2d(channels_block_3)\n",
    "        )\n",
    "\n",
    "        # fourth conv block. 3 convs. from 256 to 512 features map from 3x3 kernels. 50% downsampling and normalization at the end.\n",
    "        self.convBlock4 = nn.Sequential(nn.Conv2d(channels_block_3,channels_block_4,(3,3), padding =1,),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(channels_block_4,channels_block_4,(3,3), padding = 1),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(channels_block_4,channels_block_4,(3,3), padding = 1), #50% downsampling achieved with a 2 stride. \n",
    "                nn.ReLU(True),\n",
    "                nn.BatchNorm2d(channels_block_4)\n",
    "        )\n",
    "        \n",
    "        #fifth conv block. 3 convs. no change in nb feature maps. 3x3 kernels with 2 dilation and 2 padding to not downscale. normalization at the end.\n",
    "\n",
    "        self.convBlock5 = nn.Sequential(nn.Conv2d(channels_block_4,channels_block_5,(3,3),dilation=2,padding=2),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(channels_block_5,channels_block_5,(3,3),dilation=2,padding=2),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(channels_block_5,channels_block_5,(3,3),dilation=2,padding=2), #50% downsampling achieved with a 2 stride. \n",
    "                nn.ReLU(True),\n",
    "                nn.BatchNorm2d(channels_block_5)\n",
    "        )\n",
    "\n",
    "        #sixth conv block. same as 5\n",
    "        self.convBlock6 = nn.Sequential(nn.Conv2d(channels_block_5,channels_block_6,(3,3),dilation=2,padding=2),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(channels_block_6,channels_block_6,(3,3),dilation=2,padding=2),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(channels_block_6,channels_block_6,(3,3),dilation=2,padding=2), #50% downsampling achieved with a 2 stride. \n",
    "                nn.ReLU(True),\n",
    "                nn.BatchNorm2d(channels_block_6)\n",
    "        )\n",
    "\n",
    "        #seventh conv block : 3 convs with 3x3 kernels.\n",
    "        self.convBlock7 = nn.Sequential(nn.Conv2d(channels_block_6,channels_block_7,(3,3),padding =1,),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(channels_block_7,channels_block_7,(3,3),padding =1,),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(channels_block_7,channels_block_7,(3,3),padding =1,), #50% downsampling achieved with a 2 stride. \n",
    "                nn.ReLU(True),\n",
    "                nn.BatchNorm2d(channels_block_7)\n",
    "        )\n",
    "\n",
    "        #eighth conv block : 1 inverse conv to upsample then 2 convs with 3x3 kernels default parameters. Final convolution with 1x1 for classification into a colour bin\n",
    "        self.convBlock8 = nn.Sequential(nn.ConvTranspose2d(channels_block_7,channels_block_8,(4,4),stride = 2, padding =1),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(channels_block_8,channels_block_8,(3,3),padding = 1),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(channels_block_8,channels_block_8,(3,3),padding = 1), #50% downsampling achieved with a 2 stride. \n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(channels_block_8,nb_colour_bins,kernel_size=1) #1x1 kernel for classification in each colour bin (value will be soft maxed for probability)\n",
    "        )\n",
    "\n",
    "        self.outputLayer = nn.Conv2d(nb_colour_bins,2,kernel_size=1,dilation=1, bias = False) # 1x1 kernel to get 2 channel values of a and b respectively\n",
    "    \n",
    "    def forward(self, luminosity_image):\n",
    "        h1 = self.convBlock1((luminosity_image-self.l_cent)/self.l_norm) #normalize luminosity to be on scale of 0 to 100\n",
    "        h2 = self.convBlock2(h1)\n",
    "        h3 = self.convBlock3(h2)\n",
    "        h4 = self.convBlock4(h3)\n",
    "        h5 = self.convBlock5(h4)\n",
    "        h6 = self.convBlock6(h5)\n",
    "        h7 = self.convBlock7(h6)\n",
    "        h8 = self.convBlock8(h7)\n",
    "\n",
    "        colour_bin_proba = (nn.Softmax(dim=1))(h8)\n",
    "        output = self.outputLayer(colour_bin_proba)\n",
    "        upscaled_output = (nn.Upsample(scale_factor=4, mode='bilinear'))(output) # bilinear upscale to agree with input image size \n",
    "\n",
    "        return upscaled_output * self.ab_norm # denormalize to cover whole ab value range\n",
    "        \n",
    "colCNN = ColorizationCNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADAM\n",
    "initial_lr = 3e-5\n",
    "optimizer = torch.optim.Adam(colCNN.parameters(),lr = initial_lr , weight_decay=1e-3)\n",
    "# lr will be decreased further when loss plateaus in the training loop. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert a true pixel Y to a distribution Z (soft encoding)\n",
    "\n",
    "# to implement...\n",
    "def getColorDistribution(Y):\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert a distribution Z to a true pixel Y (point estimate)\n",
    "\n",
    "# to implement...\n",
    "def getColorEstimate(Z):\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v(Z_hw) weight in paper (section 2)\n",
    "\n",
    "def getPixelsWeights(Z):\n",
    "    W = torch.argmax(Z, dim=2)\n",
    "    for i in range(W.size(dim=0)):\n",
    "        for j in range(W.size(dim=1)):\n",
    "            W[i,j] = pixelsWeights(W[i,j])\n",
    "\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function \n",
    "\n",
    "def multinomialCrossEntropyLoss(Z_estimate, Z):\n",
    "    W = getPixelsWeights(Z)\n",
    "    L = - sum(W * torch.sum(Z * torch.log10(Z_estimate), dim=2))\n",
    "\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a trining step: prediction, loss, backprop loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(optim,loss_fct,network,dataloader):\n",
    "\n",
    "    iteration_losses = []\n",
    "\n",
    "    for image in dataloader:\n",
    "\n",
    "        predicted_colour_probability = network.forward(image)\n",
    "        loss = loss_fct(predicted_colour_probability,getColorDistribution(data))\n",
    "        iteration_losses.append(loss.item())\n",
    "\n",
    "        optim.zero_grad() \n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "    return sum(iteration_losses) / len(iteration_losses) # average loss of training iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main training loop and additional function TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_steps = 1000\n",
    "losses = np.zeros(nb_steps)\n",
    "\n",
    "imageLoader = torch.utils.data.DataLoader( dataset , batch_size=100, shuffle=True, num_workers=2)\n",
    "\n",
    "for step in range(nb_steps):\n",
    "    print('step :',step,end='\\r')\n",
    "    losses[step] = training_step(optimizer,multinomialCrossEntropyLoss,colCNN,imageLoader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = range(1,nb_steps)\n",
    "plt.plot(x_axis,losses, label=\"train loss\")\n",
    "\n",
    "plt.xlabel('number of training steps')\n",
    "plt.ylabel('avg loss')\n",
    "plt.title('multinomial cross entropy loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the neural network colorization of a specific image vs the real colors of that image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('iml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3656675e5c9ddbad44bbaefbc4c978fb0abed373f282a0307983d4ade1822146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
