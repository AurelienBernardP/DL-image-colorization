{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image colorization project :\n",
    "We train a CNN to take in greyscale images of ... and output their colorful and plausible colorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports required :\n",
    "\n",
    "torch\n",
    "\n",
    "skimage ? only used for rgb - lab parsing. Maybe we can do that ourselves to make some differentiation with the original project?\n",
    "\n",
    "numpy\n",
    "\n",
    "matplotlib\n",
    "\n",
    "PIL ? only used to open images to rgb. We can use another library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from skimage import color\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data, augment it, transform it to LAB compute stats on colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data loading\n",
    "from torchvision import datasets, transforms, utils\n",
    "from torchvision.datasets import folder, ImageFolder\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(255),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.PILToTensor(),\n",
    "])\n",
    "\n",
    "class CustomImageDataset(ImageFolder):\n",
    "    def __init__(self,root, transform = None,):\n",
    "        super().__init__(root,transform=transform)\n",
    "\n",
    "    def __len__(self):\n",
    "        return super().__len__()\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (sample, target) where target is class_index of the target class.\n",
    "        \"\"\"\n",
    "        path, target = self.samples[index]\n",
    "        sample = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return sample, index\n",
    "\n",
    "dataset = CustomImageDataset(\"cat_dataset/training_set\",transform=transform)\n",
    "batch_size = 32\n",
    "loader = data.DataLoader(dataset, batch_size = batch_size, shuffle = True)\n",
    "print(\"dataset = \",dataset)\n",
    "images, indexes = next(iter(loader))\n",
    "\n",
    "\n",
    "img = images[0]\n",
    "plt.imshow(transforms.functional.to_pil_image(img))\n",
    "plt.show()\n",
    "print(img.shape)\n",
    "img2 = img.numpy()\n",
    "\n",
    "\n",
    "#for i, (images, labels) in enumerate(loader, 0):\n",
    "   #print(images.size())\n",
    "    #sample_fname, _ = loader.dataset.samples[i]\n",
    "   # print(sample_fname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LAB transform\n",
    "\n",
    "def rgb2lab(img):\n",
    "    #INPUT  RGB image of shape (...,3,H,W)\n",
    "    #OUTPUT LAB image of shape (...,H,W,3) \n",
    "\n",
    "    lab_imgs = color.rgb2lab(img,channel_axis = 0)\n",
    "    return np.transpose(lab_imgs,(1,2,0))\n",
    "\n",
    "print(rgb2lab(img2).shape) \n",
    "print(rgb2lab(img2)) \n",
    "\n",
    "def separateChannels(imageLAB):\n",
    "    #Input : LAB image with format (H,W,3)\n",
    "    l = imageLAB[:,:,0]\n",
    "    a = imageLAB[:,:,1]\n",
    "    b = imageLAB[:,:,2]\n",
    "    return l,a,b\n",
    "\n",
    "l,a,b = separateChannels(rgb2lab(img2))\n",
    "plt.imshow(l)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Color stats (auxiliary functions)\n",
    "\n",
    "def getDiscretisedColor(a,b,gridSize):\n",
    "    a = np.round(a/gridSize) * gridSize\n",
    "    b = np.round(b/gridSize) * gridSize\n",
    "    return (a,b)\n",
    "\n",
    "def getMatrixIndex(a,b,gridSize):\n",
    "    i = (a + 500) / gridSize\n",
    "    j = (b + 200) / gridSize\n",
    "    return (int(i),int(j))\n",
    "\n",
    "def getColorValue(i,j,gridSize):\n",
    "    a = i * gridSize - 500\n",
    "    b = j * gridSize - 200\n",
    "    return (a,b)\n",
    "\n",
    "def getNNearestNeigbhors(a,b,gridSize): # to improve \"getColorDistribution\"\n",
    "    # ... to do\n",
    "    #return list of (a,b)\n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Color stats\n",
    "\n",
    "# need dataset express like that: list of images (as tensor) with dim H x W x 2 ...\n",
    "#image_test = rgb2lab(img2) # need to change dim order of color stat code... ?\n",
    "#images = [np.random.rand(3,3,2) * 100]\n",
    "#images = [image_test[:][:][1:2]]\n",
    "#print(image_test.shape)\n",
    "#print(images[0])\n",
    "\n",
    "# Initialise the proba distribution of ab pairs in the images dataset (discretised).\n",
    "gridSize = 10 # too big ?\n",
    "colorProbabilities = np.zeros((1000 // gridSize, 400 // gridSize))\n",
    "\n",
    "# Compute the proba distribution of the ab pairs in the images dataset (discretised).\n",
    "nbOfAnalysedPixels = 0\n",
    "for data in loader:\n",
    "    images,_ = data\n",
    "    for image in images: \n",
    "        image = image.numpy()\n",
    "        image = rgb2lab(image)[:][:][1:2]\n",
    "        for h in range (image.shape[0]):\n",
    "            for w in range (image.shape[1]):\n",
    "                (a,b) = getDiscretisedColor(image[h][w][0],image[h][w][1],gridSize)\n",
    "                (i,j) = getMatrixIndex(a,b,gridSize)\n",
    "                colorProbabilities[i][j] += 1\n",
    "                nbOfAnalysedPixels += 1\n",
    "colorProbabilities = colorProbabilities / nbOfAnalysedPixels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# + display distribution in 2d plot ? like in paper (here, very simple)\n",
    "plt.imshow(colorProbabilities, interpolation='none')\n",
    "plt.xlim(10, 30) # need to be dynamic !\n",
    "plt.ylim(40, 70)\n",
    "plt.show()\n",
    "\n",
    "# Smooth the proba distribution of the ab pairs in the images dataset.\n",
    "sigma = 3 # gaussian kernel parameter (test with 1 and no treshold ?)\n",
    "colorProbabilities_smooth = gaussian_filter(colorProbabilities, sigma=sigma) # is it ok ? add a lot of value in gamut... take proba treshold ?\n",
    "treshold = 0.001 # add treshold if smoothing\n",
    "\n",
    "# + display distribution in 2d plot ? like in paper (here, very simple)\n",
    "plt.imshow(colorProbabilities_smooth, interpolation='none')\n",
    "plt.xlim(10, 30) # need to be dynamic !\n",
    "plt.ylim(40, 70)\n",
    "plt.show()\n",
    "\n",
    "# Get the vector of proba of ab pairs that are \"in gamut\"\n",
    "inGamutColors = []\n",
    "inGamutColorsProbas = []\n",
    "inGamutIndex = {}\n",
    "currentColorIndex = 0\n",
    "for i in range (colorProbabilities_smooth.shape[0]):\n",
    "    for j in range (colorProbabilities_smooth.shape[1]):\n",
    "        currentColorPorba = colorProbabilities_smooth[i][j]\n",
    "        if currentColorPorba > treshold: # put a treshold ? if we use smoothing \n",
    "            (a,b) = getColorValue(i,j,gridSize)\n",
    "            inGamutColors.append((a,b))\n",
    "            inGamutColorsProbas.append(currentColorPorba)\n",
    "            inGamutIndex[(a,b)] = currentColorIndex\n",
    "            currentColorIndex += 1\n",
    "        \n",
    "Q = len(inGamutColors) \n",
    "p_smooth = torch.tensor(inGamutColorsProbas) # + need to normalise ? because treshold\n",
    "print(Q)\n",
    "print(inGamutColors)\n",
    "print(inGamutColorsProbas)\n",
    "print(inGamutIndex)\n",
    "#Q = 1 # to remove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class rebalancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define pixel weight vector (class rebalancing)\n",
    "\n",
    "# Set the parameters (from paper, need empirical value).\n",
    "lambda_uniform = 1/2 \n",
    "\n",
    "# Compute the weight vector.\n",
    "pixelsWeights = torch.reciprocal((1 - lambda_uniform) * p_smooth + lambda_uniform / Q)\n",
    "\n",
    "# Normalise the weight vector according to p_smooth (E[W] = 1).\n",
    "E_W = torch.sum(p_smooth * pixelsWeights)\n",
    "scale_factor = 1 / E_W\n",
    "pixelsWeights = scale_factor * pixelsWeights \n",
    "\n",
    "print(pixelsWeights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and instantiate Convolutional NN consistent with the description of the paper. Shown in table 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NN def\n",
    "\n",
    "class ColorizationCNN(nn.Module):\n",
    "    def __init__(self, nb_colour_bins = 313):\n",
    "        super().__init__()\n",
    "\n",
    "        self.l_cent = 50.\n",
    "        self.l_norm = 100.\n",
    "        self.ab_norm = 110.\n",
    "        \n",
    "        channels_block_1 = 64\n",
    "        channels_block_2 = 128\n",
    "        channels_block_3 = 256\n",
    "        channels_block_4 = 512\n",
    "        channels_block_5 = 512 #dilated\n",
    "        channels_block_6 = 512 #dilated\n",
    "        channels_block_7 = 512 \n",
    "        channels_block_8 = 128 # transpose convolution necessary\n",
    "\n",
    "        # first conv block : 2 convs. from luminosity image to 64 features map from 3x3 kernels. 50% downsampling and normalization at the end.\n",
    "        self.convBlock1 = nn.Sequential(nn.Conv2d(1,channels_block_1,(3,3), padding =1), \n",
    "                nn.ReLU(True), #inplace for memory efficiency can be used as no skip connections are used.\n",
    "                nn.Conv2d(channels_block_1,channels_block_1,(3,3), padding =1,stride=2), #50% downsampling achieved with a 2 stride. \n",
    "                nn.ReLU(True),\n",
    "                nn.BatchNorm2d(channels_block_1) #normalization over the 64 channels created\n",
    "        )\n",
    "\n",
    "        # second conv block. 2 covs. from 64 features to 128 features map from 3x3 kernels. 50% downsampling and normalization at the end.\n",
    "        self.convBlock2 = nn.Sequential(nn.Conv2d(64,channels_block_2,(3,3), padding =1,), \n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(channels_block_2,channels_block_2,(3,3), padding =1,stride=2), #50% downsampling achieved with a 2 stride. \n",
    "                nn.ReLU(True),\n",
    "                nn.BatchNorm2d(channels_block_2)\n",
    "        )\n",
    "\n",
    "        # third conv block. 3 convs. from 64 to 128 features map from 3x3 kernels. 50% downsampling and normalization at the end.\n",
    "        self.convBlock3 = nn.Sequential(nn.Conv2d(channels_block_2,channels_block_3,(3,3), padding = 1,), \n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(channels_block_3,channels_block_3,(3,3), padding =1),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(channels_block_3,channels_block_3,(3,3), padding =1,stride=2), #50% downsampling achieved with a 2 stride. \n",
    "                nn.ReLU(True),\n",
    "                nn.BatchNorm2d(channels_block_3)\n",
    "        )\n",
    "\n",
    "        # fourth conv block. 3 convs. from 256 to 512 features map from 3x3 kernels. 50% downsampling and normalization at the end.\n",
    "        self.convBlock4 = nn.Sequential(nn.Conv2d(channels_block_3,channels_block_4,(3,3), padding =1,),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(channels_block_4,channels_block_4,(3,3), padding = 1),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(channels_block_4,channels_block_4,(3,3), padding = 1), #50% downsampling achieved with a 2 stride. \n",
    "                nn.ReLU(True),\n",
    "                nn.BatchNorm2d(channels_block_4)\n",
    "        )\n",
    "        \n",
    "        #fifth conv block. 3 convs. no change in nb feature maps. 3x3 kernels with 2 dilation and 2 padding to not downscale. normalization at the end.\n",
    "\n",
    "        self.convBlock5 = nn.Sequential(nn.Conv2d(channels_block_4,channels_block_5,(3,3),dilation=2,padding=2),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(channels_block_5,channels_block_5,(3,3),dilation=2,padding=2),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(channels_block_5,channels_block_5,(3,3),dilation=2,padding=2), #50% downsampling achieved with a 2 stride. \n",
    "                nn.ReLU(True),\n",
    "                nn.BatchNorm2d(channels_block_5)\n",
    "        )\n",
    "\n",
    "        #sixth conv block. same as 5\n",
    "        self.convBlock6 = nn.Sequential(nn.Conv2d(channels_block_5,channels_block_6,(3,3),dilation=2,padding=2),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(channels_block_6,channels_block_6,(3,3),dilation=2,padding=2),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(channels_block_6,channels_block_6,(3,3),dilation=2,padding=2), #50% downsampling achieved with a 2 stride. \n",
    "                nn.ReLU(True),\n",
    "                nn.BatchNorm2d(channels_block_6)\n",
    "        )\n",
    "\n",
    "        #seventh conv block : 3 convs with 3x3 kernels.\n",
    "        self.convBlock7 = nn.Sequential(nn.Conv2d(channels_block_6,channels_block_7,(3,3),padding =1,),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(channels_block_7,channels_block_7,(3,3),padding =1,),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(channels_block_7,channels_block_7,(3,3),padding =1,), #50% downsampling achieved with a 2 stride. \n",
    "                nn.ReLU(True),\n",
    "                nn.BatchNorm2d(channels_block_7)\n",
    "        )\n",
    "\n",
    "        #eighth conv block : 1 inverse conv to upsample then 2 convs with 3x3 kernels default parameters. Final convolution with 1x1 for classification into a colour bin\n",
    "        self.convBlock8 = nn.Sequential(nn.ConvTranspose2d(channels_block_7,channels_block_8,(4,4),stride = 2, padding =1),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(channels_block_8,channels_block_8,(3,3),padding = 1),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(channels_block_8,channels_block_8,(3,3),padding = 1), #50% downsampling achieved with a 2 stride. \n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(channels_block_8,nb_colour_bins,kernel_size=1) #1x1 kernel for classification in each colour bin (value will be soft maxed for probability)\n",
    "        )\n",
    "\n",
    "        self.outputLayer = nn.Conv2d(nb_colour_bins,2,kernel_size=1,dilation=1, bias = False) # 1x1 kernel to get 2 channel values of a and b respectively\n",
    "    \n",
    "    def forward(self, luminosity_image):\n",
    "        h1 = self.convBlock1((luminosity_image-self.l_cent)/self.l_norm) #normalize luminosity to be on scale of 0 to 100\n",
    "        h2 = self.convBlock2(h1)\n",
    "        h3 = self.convBlock3(h2)\n",
    "        h4 = self.convBlock4(h3)\n",
    "        h5 = self.convBlock5(h4)\n",
    "        h6 = self.convBlock6(h5)\n",
    "        h7 = self.convBlock7(h6)\n",
    "        h8 = self.convBlock8(h7)\n",
    "\n",
    "        colour_bin_proba = (nn.Softmax(dim=1))(h8)\n",
    "        '''\n",
    "        output = self.outputLayer(colour_bin_proba)\n",
    "        upscaled_output = (nn.Upsample(scale_factor=4, mode='bilinear'))(output) # bilinear upscale to agree with input image size \n",
    "        upscaled_output * self.ab_norm # denormalize to cover whole ab value range\n",
    "        '''\n",
    "        return colour_bin_proba\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert a true image Y[BxHxWx2] to pixels color distributions Z[BxHxWxQ] (soft encoding)\n",
    "\n",
    "# Define a distance measure bewteen two colors (a,b).\n",
    "colorDistance = lambda c1, c2: np.sqrt((c1[0]-c2[0])**2 + (c1[1]-c2[1])**2)\n",
    "\n",
    "\n",
    "# Define a Gaussian kernel.\n",
    "def gaussianKernel(distances):\n",
    "    sigma = 5\n",
    "    weights = np.exp(-(distances**2) / sigma)\n",
    "    return weights\n",
    "\n",
    "\n",
    "def getColorDistribution(Y,nbOfNeighbors):\n",
    "    # Initiate a tensor to store the distributions produced from Y.\n",
    "    Z = torch.zeros(Y.shape[0], Y.shape[1], Y.shape[2], Q)\n",
    "\n",
    "    # Produce a color distribution for each pixel of the image.\n",
    "    for i in range (Z.shape[0]):\n",
    "        print(i)\n",
    "        for h in range (Z.shape[1]):\n",
    "            for w in range (Z.shape[2]):\n",
    "                color_true = Y[i][h][w]\n",
    "                distances = np.array([colorDistance(color,color_true) for color in inGamutColors]) # not efficient... possible to do better ?\n",
    "                nearestNeighborsIndexs = (-distances).argsort()[:nbOfNeighbors]\n",
    "                #nearestNeighborsIndexs = np.argpartition(distances, -nbOfNeighbors)[-nbOfNeighbors:]\n",
    "                weights = torch.from_numpy(gaussianKernel(distances[nearestNeighborsIndexs]))\n",
    "                for j in range (weights.shape[0]):\n",
    "                    Z[i][h][w][nearestNeighborsIndexs[j]] = weights[j]\n",
    "    \n",
    "    # Return the produced distributions.\n",
    "    return Z\n",
    "    \n",
    "\n",
    "def getColorDistribution_1hot(Y):\n",
    "    # Initiate a tensor to store the distributions produced from Y.\n",
    "    Z = torch.zeros(Y.shape[0], Y.shape[1], Y.shape[2], Q)\n",
    "\n",
    "    # Produce a color distribution for each pixel of the image.\n",
    "    for i in range (Z.shape[0]):\n",
    "        for h in range (Z.shape[1]):\n",
    "            for w in range (Z.shape[2]):\n",
    "                color_true = Y[i,h,w]\n",
    "                a, b = getDiscretisedColor(color_true[0],color_true[1],gridSize)\n",
    "                if (int(a),int(b)) in inGamutIndex:\n",
    "                    gamutIndex = inGamutIndex[(int(a),int(b))] # bug, not always in gamut... due to pooling of Y ?\n",
    "                    Z[i,h,w,gamutIndex] = 1\n",
    "                else:\n",
    "                    Z[i,h,w,:] = Z[i,h,w,:] + 1/Q\n",
    "                    print(\"not in gamut\")\n",
    "                    #print(Z[i,h,w,:])\n",
    "    \n",
    "    # Return the produced distributions.\n",
    "    return Z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the pixel color distributions in Z[HxWxQ] to true picture estimate Y[HxWx2] (point estimate)\n",
    "\n",
    "# No need to be very efficient because only use when we predict (after training) ? difficult to rewrite with tensor operation\n",
    "\n",
    "def getPictureEstimate(Z,T):\n",
    "    # Initiate a tensor to store the image estimated from Z.\n",
    "    Y_estimate = torch.zeros(Z.shape[0], Z.shape[1], 2)\n",
    "    \n",
    "    # Estimate the Lab color for each pixel of the image.\n",
    "    for h in range (Y_estimate.shape[0]):\n",
    "        for w in range (Y_estimate.shape[1]):\n",
    "            # Re-ajust the temperture of the current distribution.\n",
    "            reajustedDistribution = torch.exp(torch.log10(Z[h,w,:]) / T)  / torch.sum(torch.exp(torch.log10(Z[h,w,:]) / T)) # check again...\n",
    "\n",
    "            # Compute the anneled-mean of the current distribution. \n",
    "            a, b = 0, 0\n",
    "            for q in range (Z.shape[2]):\n",
    "                a += reajustedDistribution[q] * inGamutColors[q][0]\n",
    "                b += reajustedDistribution[q] * inGamutColors[q][1]\n",
    "            \n",
    "            # Estimate the Lab color for the current pixel.\n",
    "            Y_estimate[h][w][0] = a\n",
    "            Y_estimate[h][w][1] = b\n",
    "    \n",
    "    # Return the estimated picture.\n",
    "    return Y_estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take Z[BxHxWxQ] as input where B is the batch size\n",
    "\n",
    "#v(Z_hw) weight in paper (section 2)\n",
    "\n",
    "def getPixelsWeights(Z_batch):\n",
    "    W = torch.argmax(Z_batch, dim=3)\n",
    "    for i in range(W.size(dim=0)):\n",
    "        for h in range(W.size(dim=1)):\n",
    "            for w in range(W.size(dim=2)):\n",
    "                W[i,h,w] = pixelsWeights[W[i,h,w]]\n",
    "\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take Z[BxHxWxQ] as input where B is the batch size\n",
    "\n",
    "#loss function \n",
    "\n",
    "def multinomialCrossEntropyLoss(Z_estimate_batch, Z_batch):\n",
    "    W = getPixelsWeights(Z_batch)\n",
    "    L = - torch.sum(torch.sum(torch.sum(W * torch.sum(Z_batch * torch.log10(Z_estimate_batch + sys.float_info.epsilon), dim=3), dim=2), dim=1))\n",
    "\n",
    "    return L\n",
    "\n",
    "#Loss = multinomialCrossEntropyLoss(torch.rand(4,224,224,Q), torch.rand(4,224,224,Q)) # test\n",
    "#print(Loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main training loop and additional function TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft encoding (pre computation of the distributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_encoded_images = {}\n",
    "\n",
    "for data in loader:\n",
    "    images, indexes = data\n",
    "\n",
    "    #print(indexes.shape[0])\n",
    "\n",
    "    lab_images = []\n",
    "\n",
    "    for i in range(images.shape[0]):\n",
    "        lab_im = rgb2lab(images[i].numpy())\n",
    "        lab_images.append(lab_im)\n",
    "\n",
    "    lab_images = torch.tensor(np.array(lab_images))\n",
    "\n",
    "    downsampler_to_quarter_size = nn.AvgPool2d(4, stride=4)\n",
    "    downsampled_lab = downsampler_to_quarter_size(torch.permute(lab_images,(0,3,1,2)))\n",
    "    downsampled_lab = torch.permute(downsampled_lab,(0,2,3,1))\n",
    "\n",
    "    #print(downsampled_lab[:,:,:,1:3].shape)\n",
    "        \n",
    "    imagesDistributions = getColorDistribution_1hot(downsampled_lab[:,:,:,1:3])\n",
    "\n",
    "    for i in range(indexes.shape[0]):\n",
    "        soft_encoded_images[indexes[i].item()] = imagesDistributions[i][:][:][:].numpy()\n",
    "        \n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#soft_encoded_images[265]\n",
    "\n",
    "#a_file = open(\"soft_encoded_images_1hot_cat2.pkl\", \"wb\")\n",
    "#pickle.dump(soft_encoded_images, a_file)\n",
    "#a_file.close()\n",
    "\n",
    "#a_file = open(\"soft_encoded_images_1hot_cat2.pkl\", \"rb\")\n",
    "#soft_encoded_images = pickle.load(a_file)\n",
    "#print(output)\n",
    "#a_file.close()\n",
    "\n",
    "\n",
    "\n",
    "#a_file = open(\"soft_encoded_images_1hot_cat2.json\", \"w\")\n",
    "#json.dump(soft_encoded_images, a_file)\n",
    "#a_file.close()\n",
    "\n",
    "\n",
    "#a_file = open(\"soft_encoded_images_1hot_cat2.json\", \"r\")\n",
    "#output = a_file.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a training step: for each image in a batch : prediction, loss, backprop loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(optim,loss_fct,network,dataloader):\n",
    "    '''\n",
    "        Function to execute a single training step\n",
    "            Input : \n",
    "                optim = pytorch optimize used for the training step\n",
    "                loss_fct = loss function used to compare the predicted output to ground truth\n",
    "                dataloader = pytorch data loader to provide the training data used in the training step\n",
    "\n",
    "            Output :\n",
    "                average loss over the training step\n",
    "\n",
    "    '''\n",
    "    iteration_losses = []\n",
    "    analysed_images = 0\n",
    "\n",
    "    for data in dataloader:\n",
    "        images, indexes = data\n",
    "\n",
    "        lab_images_distributions = []\n",
    "        l_images = []\n",
    "\n",
    "        for i in range(images.shape[0]):\n",
    "            lab_im = rgb2lab(images[i].numpy())\n",
    "            lab_images_distributions.append(soft_encoded_images[indexes[i].item()]) # get pre-computed distributions (soft encoding)\n",
    "            #print(soft_encoded_images[indexes[i].item()].shape)\n",
    "            l_images.append(lab_im[:,:,0])\n",
    "       \n",
    "\n",
    "        lab_images_distributions = torch.tensor(np.array(lab_images_distributions)).to(\"cuda\")\n",
    "        #print(lab_images_distributions.shape)\n",
    "\n",
    "        l_images = torch.tensor(np.array(l_images))\n",
    "        l_images = torch.unsqueeze(l_images,dim=1).to(\"cuda\")\n",
    "\n",
    "        predicted_colour_probability = network.forward(l_images.float().to(\"cuda\"))\n",
    "        predicted_colour_probability = torch.permute(predicted_colour_probability,(0,3,2,1))\n",
    "        \n",
    "        loss = loss_fct(predicted_colour_probability, lab_images_distributions)\n",
    "        iteration_losses.append(loss.item())\n",
    "\n",
    "        optim.zero_grad() \n",
    "        loss.backward()\n",
    "        #torch.nn.utils.clip_grad_norm_(network.parameters(),10) # clip gradients to 10\n",
    "        optim.step()\n",
    "        \n",
    "        analysed_images += batch_size\n",
    "        print(f\"fraction of the dataset analysed: {round(analysed_images/3829 * 100, 2)}%\")\n",
    "\n",
    "    return sum(iteration_losses) / len(iteration_losses) # average loss of training iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colCNN = ColorizationCNN(nb_colour_bins = Q).to(\"cuda\")\n",
    "\n",
    "# to retrain a trained model\n",
    "PATH = os.getcwd() + \"/network_cat_370_epochs_4_8.pth\"\n",
    "network = ColorizationCNN(nb_colour_bins = Q)\n",
    "network.load_state_dict(torch.load(PATH))\n",
    "network.eval()\n",
    "colCNN = network.to(\"cuda\")\n",
    "#initial_lr = 3e-5 Put current learning rate ? save it\n",
    "\n",
    "initial_lr = 0.00001\n",
    "optimizer = torch.optim.Adam(colCNN.parameters(),lr = initial_lr , weight_decay=0)\n",
    "#loss_plateau_scheduler1 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, threshold = 20, factor = 0.3, verbose=True, patience= 2)\n",
    "#loss_plateau_scheduler2 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, threshold = 2,  factor = 0.3, verbose=True, patience = 2)\n",
    "nb_epochs = 100\n",
    "losses = np.zeros(nb_epochs)\n",
    "\n",
    "for epoch in range(nb_epochs):\n",
    "    print('epoch :',epoch)\n",
    "    losses[epoch] = training_step(optimizer,multinomialCrossEntropyLoss,colCNN,loader)\n",
    "    print('current loss :',losses)\n",
    "    #if nb_epochs in {10,20,30,40,50,60,70,80,90,100}:\n",
    "        #print(\"network saved\")\n",
    "        #PATH = os.getcwd() + f\"/network_cat_{nb_epochs}_epochs_1.pth\"\n",
    "        #torch.save(colCNN.to('cpu').state_dict(), PATH) \n",
    "    #loss_plateau_scheduler1.step(losses[epoch])\n",
    "    #loss_plateau_scheduler2.step(losses[epoch])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#del iteration_losses\n",
    "#learn.destroy() \n",
    "#torch.cuda.empty_cache()\n",
    "\n",
    "#del colCNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(losses)\n",
    "\n",
    "x_axis = range(1,nb_epochs+1)\n",
    "plt.plot(x_axis,losses, label=\"train loss\")\n",
    "\n",
    "plt.xlabel('number of training steps')\n",
    "plt.ylabel('avg loss')\n",
    "plt.title('multinomial cross entropy loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "for p in optimizer.param_groups:\n",
    "    print(p[\"lr\"])\n",
    "\n",
    "3e-5 - 10 * 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the neural network colorization of a specific image vs the real colors of that image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(img):\n",
    "    plt.imshow(transforms.functional.to_pil_image(img))\n",
    "    plt.show()\n",
    "\n",
    "def show_col_vs_truth(network,data_loader,batch_size):\n",
    "    '''\n",
    "        Function to output figure with one batch of colored images vs its ground truth\n",
    "        Inputs: \n",
    "            -network : Neural network for a*b* channel value predictions\n",
    "            -data_loader : pytorch data loader to load a batch of images\n",
    "\n",
    "        Outputs:\n",
    "            - Figure of colored images vs ground truth\n",
    "    '''\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Load RGB images.\n",
    "        images, indexes = next(iter(data_loader))\n",
    "\n",
    "        #show_images(utils.make_grid(images,nrow=batch_size))\n",
    "\n",
    "        # Get l channel from RGB images.\n",
    "        l_images = []\n",
    "        for i in range(batch_size):\n",
    "            lab_im = rgb2lab(images[i].numpy())\n",
    "            l_images.append(lab_im[:,:,0])\n",
    "\n",
    "        # Get a tensor of shape (Batch,1,H,W) out of the l images.\n",
    "        l_images = torch.tensor(np.array(l_images))\n",
    "        l_images = torch.unsqueeze(l_images,dim=1)\n",
    "\n",
    "        # Predictinng the colour bin probability yields tensor of size (B,Q,H,W).\n",
    "        predicted_colour_probability = network.forward(l_images.float())\n",
    "\n",
    "        # Permute prediction to get (B,H,W,Q) to later get colour estimate.\n",
    "        predicted_colour_probability = torch.permute(predicted_colour_probability,(0,2,3,1)) # or (0,3,2,1) ? or it is the same ?\n",
    "\n",
    "        predicted_rgb_images = []\n",
    "        for i in range(batch_size):\n",
    "\n",
    "            # Get estimate in formate (H,W,2). format (H,W,Q) required.\n",
    "            estimate = getPictureEstimate(predicted_colour_probability[i,:,:,:], 0.38)\n",
    "            \n",
    "            # Permute estimate to get (2,H,W) format.\n",
    "            estimate = torch.permute(estimate,(2,0,1))\n",
    "            \n",
    "            # Define upsampler.\n",
    "            bilinear_upsampler_to_224 = torch.nn.Upsample((224,224),mode='bilinear',align_corners=True)\n",
    "\n",
    "            # Upsample. to get (2,224,224) format\n",
    "            estimate = bilinear_upsampler_to_224(torch.unsqueeze(estimate, dim=0))[0,:,:,:]\n",
    "            \n",
    "            # Artificial modification, to solve issue.\n",
    "            estimate = torch.permute(estimate,(0,2,1))\n",
    "\n",
    "            # Fuse predicted channels and light channel. \n",
    "            predicted_lab_image = torch.cat((l_images[i,:,:,:],estimate), dim = 0)\n",
    "            #permute to get (224,224,3)\n",
    "            predicted_lab_image = predicted_lab_image.permute(1,2,0)\n",
    "\n",
    "            # Convert to RGB and add to list. in format(3,224,224)\n",
    "            predicted_rgb_images.append(color.lab2rgb(predicted_lab_image).transpose(2,0,1)) \n",
    "            \n",
    "        # Transform list to tensor.\n",
    "        predicted_rgb_images = torch.tensor(np.asarray(predicted_rgb_images))\n",
    "\n",
    "        #print(images.shape)\n",
    "        #print(predicted_rgb_images.shape)\n",
    "        \n",
    "        show_images(utils.make_grid(images,nrow=batch_size))\n",
    "\n",
    "        show_images(utils.make_grid(torchvision.transforms.Grayscale()(images),nrow=batch_size))\n",
    "\n",
    "        show_images(utils.make_grid(predicted_rgb_images,nrow=batch_size))\n",
    "\n",
    "        # Concatenate all images with ground truth first, then predictions.\n",
    "        #images = torch.cat((images,predicted_rgb_images),dim=0) # don't seems to work\n",
    "\n",
    "        # Show grid of images.\n",
    "        #show_images(utils.make_grid(images,nrow=batch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.getcwd() + \"/network_cat_370_epochs_4_8.pth\"\n",
    "torch.save(colCNN.to('cpu').state_dict(), PATH)   #to save a network\n",
    "\n",
    "# Load the network from file\n",
    "#network = ColorizationCNN(nb_colour_bins = Q)\n",
    "#network.load_state_dict(torch.load(PATH))\n",
    "#network.eval()\n",
    "\n",
    "network = colCNN.to('cpu')\n",
    "\n",
    "dataset_for_test = CustomImageDataset(\"cat_dataset/training_set\",transform=transform)\n",
    "loader_for_test = data.DataLoader(dataset, batch_size = 4, shuffle = True)\n",
    "\n",
    "dataset_for_test_new = CustomImageDataset(\"cat_dataset/testing_set\",transform=transform)\n",
    "loader_for_test_new = data.DataLoader(dataset_for_test_new, batch_size = 4, shuffle = True)\n",
    "\n",
    "show_col_vs_truth(network,loader_for_test, 4)\n",
    "\n",
    "show_col_vs_truth(network,loader_for_test_new, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log10(sys.float_info.epsilon)\n",
    "sys.float_info.epsilon\n",
    "\n",
    "#torch.cuda.empty_cache()\n",
    "\n",
    "#network = colCNN\n",
    "\n",
    "for p in optimizer.param_groups:\n",
    "    print(p[\"lr\"])\n",
    "\n",
    "#a_file = open(\"network_100.pkl\", \"wb\")\n",
    "#pickle.dump(network, a_file)\n",
    "#a_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load RGB images.\n",
    "#images, indexes = next(iter(loader))\n",
    "\n",
    "#show_images(utils.make_grid(images,nrow=batch_size))\n",
    "\n",
    "#lab_images_distributions = []\n",
    "#for i in range(images.shape[0]):\n",
    "  #  lab_images_distributions.append(soft_encoded_images[indexes[i].item()]) # get pre-computed distributions (soft encoding)\n",
    "   # print(soft_encoded_images[indexes[i].item()].shape)\n",
    "\n",
    "\n",
    "'''\n",
    "        \n",
    "# Load RGB images.\n",
    "images, indexes = next(iter(loader))\n",
    "\n",
    "show_images(utils.make_grid(images,nrow=batch_size))\n",
    "\n",
    "# Get l channel from RGB images.\n",
    "l_images = []\n",
    "predicted_rgb_images = []\n",
    "for i in range(batch_size):\n",
    "    lab_im = rgb2lab(images[i].numpy())\n",
    "    l_images.append(lab_im[:,:,0])\n",
    "\n",
    "    # Get a tensor of shape (Batch,1,H,W) out of the l images.\n",
    "    l_images = torch.tensor(np.array(l_images))\n",
    "    l_images = torch.unsqueeze(l_images,dim=1)\n",
    "    \n",
    "    # Estimate true image.\n",
    "    soft_encoded_image = soft_encoded_images[indexes[i].item()]\n",
    "\n",
    "    # Get estimate in formate (H,W,2). format (H,W,Q) required.\n",
    "    estimate = getPictureEstimate(torch.tensor(soft_encoded_image), 0.38)\n",
    "            \n",
    "    # Permute estimate to get (2,H,W) format.\n",
    "    estimate = torch.permute(estimate,(2,0,1))\n",
    "            \n",
    "    # Define upsampler.\n",
    "    bilinear_upsampler_to_224 = torch.nn.Upsample((224,224),mode='bilinear',align_corners=True)\n",
    "\n",
    "    # Upsample.\n",
    "    estimate = bilinear_upsampler_to_224(torch.unsqueeze(estimate, dim=0))[0,:,:,:]\n",
    "\n",
    "    # Fuse predicted channels and light channel.\n",
    "    predicted_lab_image = torch.cat((l_images[i,:,:,:],estimate), dim = 0)\n",
    "    predicted_lab_image = predicted_lab_image.permute(2,1,0)\n",
    "\n",
    "    # Convert to RGB and add to list.\n",
    "    predicted_rgb_images.append(color.lab2rgb(predicted_lab_image).transpose(2,1,0))\n",
    "\n",
    "\n",
    "# Transform list to tensor.\n",
    "predicted_rgb_images = torch.tensor(np.asarray(predicted_rgb_images))\n",
    "\n",
    "print(images.shape)\n",
    "print(predicted_rgb_images.shape)\n",
    "        \n",
    "show_images(utils.make_grid(images,nrow=batch_size))\n",
    "    \n",
    "show_images(utils.make_grid(predicted_rgb_images,nrow=batch_size))\n",
    "'''\n",
    "    \n",
    "\n",
    "\n",
    "losses = [81303.28811849,61758.39309896,60933.99667969,60846.84921875, \n",
    "60542.11933594,60462.38883464,60337.95807292,60110.13154297,\n",
    "60108.94763997,59988.05817057,59838.87047526,59823.27936198,\n",
    "59862.82783203,59862.36497396,59503.20800781,60385.47617188,59762.79378255,59433.95097656,59476.44599609,\n",
    "59232.75934245,59243.96279297,59289.17327474,59171.0921224,\n",
    "59111.21907552,58940.96315104,58793.42688802,58777.8258138,\n",
    "58615.96347656,58376.17075195,58321.16580404,58087.07001953,\n",
    "57749.02513021,57416.25751953,57088.15449219,56712.53020833,\n",
    "56157.13948568,55397.20680339,54623.96822917,53934.46074219,\n",
    "52274.30084635,51122.04365234,49193.14459635,47385.42890625,\n",
    "45765.42220052,43749.64643555,41399.53181966,39328.01105143,37910.80113932,36703.84072266,\n",
    "35591.55758464,34562.16321615,33845.62635091,32878.93821615,\n",
    "31838.24560547,30979.11360677,30132.19482422,29355.42911784,\n",
    "28645.84597982,28013.73204753,27258.69205729,26525.13854167,\n",
    "26080.20646159,25697.09316406,24990.82750651,24478.30818685,\n",
    "23913.17114258,23590.73916016,22796.37157389,22394.54726562,\n",
    "22202.75438639,21732.13532715,21263.52696126,20803.80828451,\n",
    "20537.92703451,20218.08628743,19662.33267415,19308.37285156,\n",
    "19077.41974284,18815.43061523,18515.57627767,18321.58497721,\n",
    "18255.83783366,18051.08092448,17616.18653971,17315.96066081,\n",
    "17042.43725586,16933.93890788,16612.2267334,16231.99694824,\n",
    "15920.14764811,15764.53597819,15789.13213704,15678.92716471,\n",
    "15457.65901693,15176.34157715,15186.51732585,15100.40015462,\n",
    "14798.8755778,14589.73163249,14438.51653239,14595.17989095,13581.99746094,13875.56790365,14093.27469076,\n",
    "14142.8011556,13910.21253255,13567.58085124,13326.37622477,\n",
    "13202.69063314,13150.1250651,12940.26027832,12669.48104655,\n",
    "12581.39984538,12591.53572184,12676.23735352,12542.54065755,\n",
    "12228.427889,12027.60822754,11967.03070068,11768.7973348,\n",
    "11741.22125651,11685.82579753,11644.20712077,11495.3114502,\n",
    "11197.87001953,11004.38503011,10918.35761719,11003.60126546,\n",
    "11006.58417562,10899.27492269,9963.85609945,9091.36595256,8675.66303711,8396.52987061,8192.35740967,\n",
    "8030.98548584,7898.40179443,7780.7162028,7674.7762736,7575.3977356,\n",
    "7481.12873942,7390.57533366,7301.74674479,7216.54933675,7132.30423991,\n",
    "7049.68236898,6970.03470459,6891.81573893,6818.14851888,6746.09878743,\n",
    "6678.90551351,6615.67351074,6550.99157308,6489.31016032,6426.28203939,\n",
    "6362.18175252,6294.28593343,6230.20939535,6170.33973592,6113.17020264,\n",
    "6050.65525309,5968.09266561,5908.25709025,5869.12924194,5839.92531331,\n",
    "5806.75832113,5757.05664266,5704.51771851,5655.94170736,5614.19325765,\n",
    "5576.30623779,5536.75324707,5498.43127848,5464.08488973,5425.73771566,\n",
    "5376.89332886,5336.58174032,5304.00399373,5272.04790039,5239.8788798,\n",
    "5205.22026774,5180.41722005,5146.02024129,5106.4541158,5065.28033854,\n",
    "5025.86262614,4990.94421387,4968.26611735,4947.32179769,4919.26311239,\n",
    "4875.10259196,4808.32457275,4771.43911743,4760.13659871,4755.83823242,\n",
    "4746.45599772,4721.23614502,4686.07113851,4657.62104289,4631.11352743,\n",
    "4611.33476766,4591.88505046,4558.91183268,4533.25922038,4510.15153198,\n",
    "4489.45515645,4468.35328979,4438.88601685,4417.72651367,4390.40007324,\n",
    "4367.53730062,4352.78095703,4341.23876546,4327.88150838,4308.23548991,\n",
    "4277.41167908,4254.63707072,4233.27871908,4212.75116374,4190.56605225,\n",
    "4166.29780477,4151.04988607,4140.67917379,4128.73316447,4106.43638916,\n",
    "4082.23192139,4061.69511312,4037.24503174,4013.0661438,4004.29313558,\n",
    "3986.12979736,3971.4168162,3952.33846741,3928.62168376,3912.52049561,\n",
    "3896.26543376,3890.35382284,3884.0096578,3870.97352295,3848.9981191,\n",
    "3816.7473053,3797.63467102,3788.28537394,3770.67056986,3750.78688253,\n",
    "3741.62823893,3725.97050578,3709.8980245,3702.84024862,3690.93632202,\n",
    "3679.71517944,3669.47938232,3646.0193339,3625.42430623,3612.77021077,\n",
    "3599.67722778,3579.03349711,3569.05132751,3556.50290731,3552.09162191,\n",
    "3546.98288066,3532.38292135,3511.45076294,3488.0407196,3480.34364726,\n",
    "3473.93495585,3454.98647359,3438.8712738,3425.41775614,3414.22795919,\n",
    "3394.53320109,3394.38105672,3358.43950806,3341.53254903,3344.06596171,\n",
    "3344.07758687,3333.41888936,3332.34603577,3319.23003642,3304.38216756,\n",
    "3286.81543681,3275.59824117,3265.48050028,3259.0771047,3255.6634023,\n",
    "3256.56322632,3246.0257843,3224.52381795,3204.63625946,3193.94968669,\n",
    "3174.66397807,3161.00380859,3153.7324117,3154.79174601,3166.57559001,\n",
    "3168.34014079,3137.95918376,3101.06948954,3081.72660828,3079.67484334]\n",
    "\n",
    "x_axis = range(1,300+1)\n",
    "plt.plot(x_axis,losses, label=\"train loss\")\n",
    "\n",
    "plt.xlabel('number of training steps')\n",
    "plt.ylabel('avg loss')\n",
    "plt.title('multinomial cross entropy loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('deep_learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "464699cf85eadd29f123288e0bb2f83c79f3ce7e20f2c87b57abaa5e0b3edf52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
